{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSNN-enhancementExperiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmgD1Ak9Yt1l"
      },
      "source": [
        "#Incorporating LS model to weight adjustment - experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqb4LEargIk7",
        "outputId": "31fd1e3c-1286-4dfb-9cfd-8da1c1fd3cb8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')   "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJzhDXtY53b"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyokSWbdY6fn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98_4iFoKY-lL"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from numpy import save, load\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZjE-FujZUMU"
      },
      "source": [
        "##Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9wkq-PBZAKh"
      },
      "source": [
        "#location to breast-cancer-wisconsin-w.csv dataset\n",
        "fileLocation = '/content/drive/My Drive/Colab Notebooks/datasets/breast-cancer/breast-cancer-wisconsin-w.csv'\n",
        "\n",
        "data = pd.read_csv(fileLocation) \n",
        "X = data.values[:, 1:10]\n",
        "y = data.values[:, 10]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFxbwH5LZRaV"
      },
      "source": [
        "#encoding target class\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYwYvgUSZuZr"
      },
      "source": [
        "## NN with LS model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SwM64aKZtNQ"
      },
      "source": [
        "class NeuralNetworkLSNN():\n",
        "    def __init__(self, n_input, n_hidden=30,\n",
        "                 epochs=100, alpha=0.5,\n",
        "                 random_state=1, enhancement=0.1, enhancement_type=\"none\"):\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = 1 #binary classifier\n",
        "\n",
        "        self.w_h, self.w_o = self._initialize_weights()\n",
        "\n",
        "        self.epochs = epochs+1\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.enhancement = enhancement\n",
        "\n",
        "        self.enhancement_type = enhancement_type\n",
        "\n",
        "        self.enhancement_functions = {\n",
        "            \"none\": self._adjustment_enh_none,\n",
        "            \"ls_value\": self._adjustment_ls_value,\n",
        "            \"ls_value_save\": self._adjustment_ls_value_save,\n",
        "            \"save_node_difference\": self._adjustment_save_node_difference,\n",
        "            \"value_node_difference\": self._adjustment_value_node_difference,\n",
        "            \"save_node_unified\": self._adjustment_save_node_unified,\n",
        "            \"value_node_unified\": self._adjustment_value_node_unified,\n",
        "            \"save_node_unified_flattened\": self._adjustment_save_node_unified_flattened,\n",
        "            \"value_node_unified_flattened\": self._adjustment_value_node_unified_flattened\n",
        "        }\n",
        "\n",
        "    def fit_eval(self, X, y, X_eval, y_eval):\n",
        "        # repeat\n",
        "        start_time = time.time()\n",
        "        \n",
        "        error_per_epoch = np.zeros((self.epochs))\n",
        "        accuracy_per_epoch = np.zeros((self.epochs))\n",
        "\n",
        "        error_per_epoch_train = np.zeros((self.epochs))\n",
        "        accuracy_per_epoch_train = np.zeros((self.epochs))\n",
        "        \n",
        "        for i in range(self.epochs):\n",
        "            error, accuracy = self._eval(X_eval, y_eval)\n",
        "            error_per_epoch[i] = error\n",
        "            accuracy_per_epoch[i] = accuracy\n",
        "            error_train, accuracy_train = self._eval(X_train, y_train)\n",
        "            accuracy_per_epoch_train[i] = accuracy_train\n",
        "\n",
        "            # iterate over training set\n",
        "            self.w_h_old = self.w_h\n",
        "            self.w_o_old = self.w_o\n",
        "            for j in range(X_train.shape[0]):\n",
        "                # target for actual input\n",
        "                target = y_train[j]\n",
        "\n",
        "                # trained input\n",
        "                X = X_train[j]\n",
        "\n",
        "                # activations\n",
        "                a1, a2, a3, z2, z3 = self._feedforward(X)\n",
        "                \n",
        "                self.enhancement_functions[self.enhancement_type](a1, a2, a3, z2, z3, target)\n",
        "\n",
        "            if (i % 50 == 0): #informative output\n",
        "              print(self.enhancement_type, ': epoch: ', i, ' time: ', (time.time() - start_time), ' error: ', error, ' accuracy: ', accuracy, ' accuracy_train: ', accuracy_train)\n",
        "\n",
        "        print(time.time() - start_time)\n",
        "        return error_per_epoch, accuracy_per_epoch, accuracy_per_epoch_train\n",
        "\n",
        "    def _adjustment_none_unified(self, a1, a2, a3, z2, z3, target, save):\n",
        "        # backpropagation - adjusting output layer weights\n",
        "        adj_o = np.zeros((self.n_hidden, self.n_output))\n",
        "\n",
        "        delta_output = self._delta_output(a3, target)\n",
        "        ls = self._loosely_symmetric(a2, z3)\n",
        "        cmp = np.greater_equal(ls, a2)\n",
        "        diff = np.abs(ls - a2)\n",
        "        n_a2 = [a2[i] * (1.0 + self.enhancement)  if  cmp[i] else a2[i] * (1.0 - self.enhancement) for i in range (a2.shape[0])]\n",
        "        if(save):\n",
        "          a2 = n_a2\n",
        "        adj_o = - self.alpha * delta_output * n_a2\n",
        "\n",
        "        adj_h = np.zeros((self.n_hidden, self.n_input))\n",
        "\n",
        "        #for each hidden node\n",
        "        for index in range(self.n_hidden):\n",
        "          delta_output_times_w_h = self.w_o[index] * self._delta_output(\n",
        "                      a3, target)\n",
        "          delta_hidden = self._delta_hidden(\n",
        "                        delta_output_times_w_h, a2[index])\n",
        "                  \n",
        "          ls = self._loosely_symmetric(a1, z2[index])\n",
        "          cmp = np.greater_equal(ls, a1)\n",
        "          diff = np.abs(ls - a1)\n",
        "          n_a1 = [a1[i] * (1.0 + self.enhancement) if  cmp[i] else a1[i] * (1.0 - self.enhancement) for i in range (a1.shape[0])]\n",
        "          # if(save):\n",
        "          #   a1 = np.asarray(n_a1)\n",
        "          adj_h[index] = - self.alpha * delta_hidden * n_a1\n",
        "\n",
        "        self.w_o += adj_o.reshape(self.w_o.shape)\n",
        "        self.w_h += adj_h.T\n",
        "\n",
        "    def _adjustment_none_unified_flattened(self, a1, a2, a3, z2, z3, target, save):\n",
        "        # backpropagation - adjusting output layer weights\n",
        "        adj_o = np.zeros((self.n_hidden, self.n_output))\n",
        "\n",
        "        delta_output = self._delta_output(a3, target)\n",
        "        ls = self._loosely_symmetric(a2, z3)\n",
        "        cmp = np.greater_equal(ls, a2)\n",
        "        diff = np.abs(ls - a2)\n",
        "        n_a2 = [(a2[i] * (1.0 + self.enhancement) if a2[i] * (self.enhancement) < diff[i] else ls[i])  if  cmp[i] else (a2[i] * (1.0 - self.enhancement) if a2[i] * (self.enhancement) < diff[i] else ls[i])  for i in range (a2.shape[0])]\n",
        "        if(save):\n",
        "          a2 = n_a2\n",
        "        adj_o = - self.alpha * delta_output * n_a2\n",
        "\n",
        "        adj_h = np.zeros((self.n_hidden, self.n_input))\n",
        "\n",
        "        #for each hidden node\n",
        "        for index in range(self.n_hidden):\n",
        "          delta_output_times_w_h = self.w_o[index] * self._delta_output(\n",
        "                      a3, target)\n",
        "          delta_hidden = self._delta_hidden(\n",
        "                        delta_output_times_w_h, a2[index])\n",
        "                  \n",
        "          ls = self._loosely_symmetric(a1, z2[index])\n",
        "          cmp = np.greater_equal(ls, a1)\n",
        "          diff = np.abs(ls - a1)\n",
        "          n_a1 = [(a1[i] * (1.0 + self.enhancement) if a1[i] * (self.enhancement) < diff[i] else ls[i])  if  cmp[i] else (a1[i] * (1.0 - self.enhancement) if a1[i] * (self.enhancement) < diff[i] else ls[i])  for i in range (a1.shape[0])]\n",
        "          if(save):\n",
        "            a1 = np.asarray(n_a1)\n",
        "          adj_h[index] = - self.alpha * delta_hidden * n_a1\n",
        "\n",
        "        self.w_o += adj_o.reshape(self.w_o.shape)\n",
        "        self.w_h += adj_h.T\n",
        "\n",
        "    def _adjustment_save_node_unified(self, a1, a2, a3, z2, z3, target):\n",
        "      self._adjustment_none_unified(a1, a2, a3, z2, z3, target, True)\n",
        "\n",
        "    def _adjustment_value_node_unified(self, a1, a2, a3, z2, z3, target):\n",
        "      self._adjustment_none_unified(a1, a2, a3, z2, z3, target, False)\n",
        "\n",
        "    def _adjustment_save_node_unified_flattened(self, a1, a2, a3, z2, z3, target):\n",
        "      self._adjustment_none_unified_flattened(a1, a2, a3, z2, z3, target, True)\n",
        "\n",
        "    def _adjustment_value_node_unified_flattened(self, a1, a2, a3, z2, z3, target):\n",
        "      self._adjustment_none_unified_flattened(a1, a2, a3, z2, z3, target, False)\n",
        "\n",
        "    def _adjustment_none_difference(self, a1, a2, a3, z2, z3, target, save):\n",
        "        # backpropagation - adjusting output layer weights\n",
        "        adj_o = np.zeros((self.n_hidden, self.n_output))\n",
        "\n",
        "        delta_output = self._delta_output(a3, target)\n",
        "        ls = self._loosely_symmetric(a2, z3)\n",
        "        n_a2 = (ls-a2)*self.enhancement + a2\n",
        "        if(save):\n",
        "          a2 = n_a2\n",
        "        adj_o = - self.alpha * delta_output * n_a2\n",
        "\n",
        "        adj_h = np.zeros((self.n_hidden, self.n_input))\n",
        "\n",
        "        #for each hidden node\n",
        "        for index in range(self.n_hidden):\n",
        "          delta_output_times_w_h = self.w_o[index] * self._delta_output(\n",
        "                      a3, target)\n",
        "          delta_hidden = self._delta_hidden(\n",
        "                        delta_output_times_w_h, a2[index])\n",
        "                  \n",
        "          ls = self._loosely_symmetric(a1, z2[index])\n",
        "          n_a1 = (ls-a1)*self.enhancement + a1\n",
        "          if(save):\n",
        "            a1 = n_a1\n",
        "          adj_h[index] = - self.alpha * delta_hidden * n_a1\n",
        "\n",
        "        self.w_o += adj_o.reshape(self.w_o.shape)\n",
        "        self.w_h += adj_h.T\n",
        "\n",
        "    def _adjustment_save_node_difference(self, a1, a2, a3, z2, z3, target):\n",
        "        self._adjustment_none_difference(a1, a2, a3, z2, z3, target, True)\n",
        "\n",
        "    def _adjustment_value_node_difference(self, a1, a2, a3, z2, z3, target):\n",
        "        self._adjustment_none_difference(a1, a2, a3, z2, z3, target, False)     \n",
        "\n",
        "    def _adjustment_ls_value(self, a1, a2, a3, z2, z3, target):\n",
        "        # backpropagation - adjusting output layer weights\n",
        "        adj_o = np.zeros((self.n_hidden, self.n_output))\n",
        "\n",
        "        delta_output = self._delta_output(a3, target)\n",
        "        ls = self._loosely_symmetric(a2, z3)\n",
        "        adj_o = - self.alpha * delta_output * ls\n",
        "\n",
        "        adj_h = np.zeros((self.n_hidden, self.n_input))\n",
        "\n",
        "        #for each hidden node\n",
        "        for index in range(self.n_hidden):\n",
        "          delta_output_times_w_h = self.w_o[index] * self._delta_output(\n",
        "                      a3, target)\n",
        "          delta_hidden = self._delta_hidden(\n",
        "                        delta_output_times_w_h, a2[index])\n",
        "                  \n",
        "          ls = self._loosely_symmetric(a1, z2[index])\n",
        "          adj_h[index] = - self.alpha * delta_hidden * ls\n",
        "\n",
        "        self.w_o += adj_o.reshape(self.w_o.shape)\n",
        "        self.w_h += adj_h.T\n",
        "\n",
        "    def _adjustment_ls_value_save(self, a1, a2, a3, z2, z3, target):\n",
        "        # backpropagation - adjusting output layer weights\n",
        "        adj_o = np.zeros((self.n_hidden, self.n_output))\n",
        "\n",
        "        delta_output = self._delta_output(a3, target)\n",
        "        a2 = self._loosely_symmetric(a2, z3)\n",
        "        adj_o = - self.alpha * delta_output * a2\n",
        "\n",
        "        adj_h = np.zeros((self.n_hidden, self.n_input))\n",
        "\n",
        "        #for each hidden node\n",
        "        for index in range(self.n_hidden):\n",
        "          delta_output_times_w_h = self.w_o[index] * self._delta_output(\n",
        "                      a3, target)\n",
        "          delta_hidden = self._delta_hidden(\n",
        "                        delta_output_times_w_h, a2[index])\n",
        "                  \n",
        "          a1 = self._loosely_symmetric(a1, z2[index])\n",
        "\n",
        "          adj_h[index] = - self.alpha * delta_hidden * a1\n",
        "\n",
        "        self.w_o += adj_o.reshape(self.w_o.shape)\n",
        "        self.w_h += adj_h.T\n",
        "\n",
        "    def _adjustment_enh_none(self, a1, a2, a3, z2, z3, target):\n",
        "        # backpropagation - adjusting output layer weights\n",
        "        adj_o = np.zeros((self.n_hidden, self.n_output))\n",
        "\n",
        "        delta_output = self._delta_output(a3, target)\n",
        "        adj_o = - self.alpha * delta_output * a2\n",
        "\n",
        "        adj_h = np.zeros((self.n_hidden, self.n_input))\n",
        "\n",
        "        #for each hidden node\n",
        "        for index in range(self.n_hidden):\n",
        "          delta_output_times_w_h = self.w_o[index] * self._delta_output(\n",
        "                      a3, target)\n",
        "          delta_hidden = self._delta_hidden(\n",
        "                        delta_output_times_w_h, a2[index])\n",
        "                  \n",
        "          adj_h[index] = - self.alpha * delta_hidden * a1\n",
        "\n",
        "        self.w_o += adj_o.reshape(self.w_o.shape)\n",
        "        self.w_h += adj_h.T\n",
        "\n",
        "    def _feedforward(self, X):\n",
        "        # input\n",
        "        a1 = X.astype(np.float)\n",
        "\n",
        "        # wieghted sum - hidden layer\n",
        "        z2 = a1.dot(self.w_h)\n",
        "        a2 = self._sigmoid(z2.astype(np.float))\n",
        "\n",
        "        # wieghted sum - output layer\n",
        "        z3 = a2.dot(self.w_o)\n",
        "        a3 = self._sigmoid(z3.astype(np.float))\n",
        "        return a1, a2, a3, z2, z3\n",
        "\n",
        "    def _delta_output(self, output, target):\n",
        "        return -(target - output) * self._sigmoid_derivative(output)\n",
        "\n",
        "    def _delta_hidden(self, delta_output_times_w_h, output):\n",
        "        return self._sigmoid_derivative(output) * delta_output_times_w_h\n",
        "\n",
        "    def _loosely_symmetric(self, a, d):\n",
        "      b =  np.ones((a.shape)) - a\n",
        "      c = 1 - d\n",
        "\n",
        "      bd = (b*d) / (b+d)\n",
        "      ac = (a*c) / (a+c)\n",
        "      return (a + bd) / (1 + ac + bd)\n",
        "\n",
        "    def _predict_value(self, X):\n",
        "        a1, a2, a3, z2, z3 = self._feedforward(X)\n",
        "        if a3 >= 0.5:\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "\n",
        "    def predict(self, X):\n",
        "        result = []\n",
        "        for x in X:\n",
        "          result.append(self._predict_value(x))\n",
        "        return result\n",
        "\n",
        "    def _activation(self, weight, layer):\n",
        "        weighted_sum = np.dot(weight, layer)\n",
        "        return self._sigmoid(weighted_sum)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        w1 = np.random.randn(self.n_input, self.n_hidden) / np.sqrt(self.n_hidden)\n",
        "        w2 = np.random.randn(self.n_hidden, self.n_output) / np.sqrt(self.n_output)\n",
        "        return w1, w2\n",
        "\n",
        "    def _accuracy_metric(self, actual, predicted):\n",
        "      correct = 0\n",
        "      for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "          correct += 1\n",
        "      return correct / float(len(actual)) * 100.0\n",
        "\n",
        "    def _error_squared(self, actual, predicted):\n",
        "      error = 0\n",
        "      for i in range(len(actual)):\n",
        "        error += (predicted[i] - actual[i])**2\n",
        "      return error * 0.5\n",
        "\n",
        "    def _eval(self, X, y):\n",
        "        result = []\n",
        "        for x in X:\n",
        "          result.append(self._predict_value(x))\n",
        "        error = self._error_squared(y, result)\n",
        "        accuracy = self._accuracy_metric(y, result)\n",
        "        return error, accuracy\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "    def _sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDTjTJDuaw4j"
      },
      "source": [
        "##K-fold cross validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LuXt1MvbEgv"
      },
      "source": [
        "accuracy_eval = {\n",
        "            \"none\": [],\n",
        "            \"ls_value\": [],\n",
        "            \"ls_value_save\": [],\n",
        "            \"save_node_difference\": [],\n",
        "            \"value_node_difference\": [],\n",
        "            \"save_node_unified\": [],\n",
        "            \"value_node_unified\": [],\n",
        "            \"save_node_unified_flattened\": [],\n",
        "            \"value_node_unified_flattened\": [],\n",
        "        }\n",
        "accuracy_train = {\n",
        "            \"none\": [],\n",
        "            \"ls_value\": [],\n",
        "            \"ls_value_save\": [],\n",
        "            \"save_node_difference\": [],\n",
        "            \"value_node_difference\": [],\n",
        "            \"save_node_unified\": [],\n",
        "            \"value_node_unified\": [],\n",
        "            \"save_node_unified_flattened\": [],\n",
        "            \"value_node_unified_flattened\": [],\n",
        "        }"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GXkQsZqbG1_"
      },
      "source": [
        "##Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ9TtAOobH3Y"
      },
      "source": [
        "types = [\"none\", \"ls_value\", \"ls_value_save\", \"save_node_difference\", \"value_node_difference\", \"save_node_unified\", \"value_node_unified\", \"save_node_unified_flattened\", \"value_node_unified_flattened\"]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he2YO6pWceIH"
      },
      "source": [
        "seed(42)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSMQMnj7chU2"
      },
      "source": [
        "X_new = copy.copy(X)\n",
        "y_new = copy.copy(y)\n",
        "X_new, y_new = shuffle(X_new, y_new)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnpA8jRTcizv"
      },
      "source": [
        "kf = KFold(n_splits=10)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WOqaSxVckbB",
        "outputId": "cfc0c090-dafc-42ee-94c4-08b9dfff6453"
      },
      "source": [
        "i = 1\n",
        "for train_index, test_index in kf.split(X_new):\n",
        "  print(i)\n",
        "  X_train, X_test = X_new[train_index], X_new[test_index]\n",
        "  y_train, y_test = y_new[train_index], y_new[test_index]\n",
        "  for item in types:\n",
        "    nn = NeuralNetworkLSNN(n_input=X_train.shape[1], n_hidden=30, epochs=100, enhancement=0.1, enhancement_type=item)\n",
        "    loss, accuracy1, accuracy2 = nn.fit_eval(X_train, y_train, X_test, y_test)\n",
        "    accuracy_eval[item].append(accuracy1)\n",
        "    accuracy_train[item].append(accuracy2)\n",
        "  i+= 1\n",
        "  break #show only first run"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "none : epoch:  0  time:  0.28562402725219727  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "none : epoch:  50  time:  14.084242820739746  error:  2.5  accuracy:  92.7536231884058  accuracy_train:  94.78827361563518\n",
            "none : epoch:  100  time:  27.65171241760254  error:  4.0  accuracy:  88.40579710144928  accuracy_train:  92.18241042345277\n",
            "27.65690040588379\n",
            "ls_value : epoch:  0  time:  0.6153035163879395  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "ls_value : epoch:  50  time:  30.550877332687378  error:  10.5  accuracy:  69.56521739130434  accuracy_train:  64.49511400651465\n",
            "ls_value : epoch:  100  time:  60.345903396606445  error:  10.5  accuracy:  69.56521739130434  accuracy_train:  64.49511400651465\n",
            "60.348490953445435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:324: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ls_value_save : epoch:  0  time:  0.6851861476898193  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "ls_value_save : epoch:  50  time:  34.293734073638916  error:  10.5  accuracy:  69.56521739130434  accuracy_train:  64.49511400651465\n",
            "ls_value_save : epoch:  100  time:  68.16679191589355  error:  10.5  accuracy:  69.56521739130434  accuracy_train:  64.49511400651465\n",
            "68.16944193840027\n",
            "save_node_difference : epoch:  0  time:  0.6602678298950195  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "save_node_difference : epoch:  50  time:  33.91011834144592  error:  24.0  accuracy:  30.434782608695656  accuracy_train:  35.50488599348534\n",
            "save_node_difference : epoch:  100  time:  67.36231970787048  error:  24.0  accuracy:  30.434782608695656  accuracy_train:  35.50488599348534\n",
            "67.36616206169128\n",
            "value_node_difference : epoch:  0  time:  0.6813650131225586  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "value_node_difference : epoch:  50  time:  33.44634437561035  error:  10.5  accuracy:  69.56521739130434  accuracy_train:  64.49511400651465\n",
            "value_node_difference : epoch:  100  time:  65.65416479110718  error:  10.5  accuracy:  69.56521739130434  accuracy_train:  64.49511400651465\n",
            "65.65451884269714\n",
            "save_node_unified : epoch:  0  time:  0.8982069492340088  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "save_node_unified : epoch:  50  time:  43.20838165283203  error:  2.5  accuracy:  92.7536231884058  accuracy_train:  95.60260586319218\n",
            "save_node_unified : epoch:  100  time:  85.52745652198792  error:  3.0  accuracy:  91.30434782608695  accuracy_train:  94.62540716612378\n",
            "85.53007078170776\n",
            "value_node_unified : epoch:  0  time:  0.8981494903564453  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "value_node_unified : epoch:  50  time:  43.19073033332825  error:  2.5  accuracy:  92.7536231884058  accuracy_train:  93.32247557003257\n",
            "value_node_unified : epoch:  100  time:  85.3359227180481  error:  6.0  accuracy:  82.6086956521739  accuracy_train:  83.55048859934854\n",
            "85.33828473091125\n",
            "save_node_unified_flattened : epoch:  0  time:  1.0334112644195557  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "save_node_unified_flattened : epoch:  50  time:  52.19031763076782  error:  2.5  accuracy:  92.7536231884058  accuracy_train:  95.60260586319218\n",
            "save_node_unified_flattened : epoch:  100  time:  103.12388467788696  error:  3.0  accuracy:  91.30434782608695  accuracy_train:  97.71986970684038\n",
            "103.12657189369202\n",
            "value_node_unified_flattened : epoch:  0  time:  0.9559431076049805  error:  27.0  accuracy:  21.73913043478261  accuracy_train:  29.47882736156352\n",
            "value_node_unified_flattened : epoch:  50  time:  49.44331121444702  error:  3.5  accuracy:  89.85507246376811  accuracy_train:  91.36807817589576\n",
            "value_node_unified_flattened : epoch:  100  time:  98.03514432907104  error:  3.0  accuracy:  91.30434782608695  accuracy_train:  89.73941368078175\n",
            "98.03806686401367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzwVJdg8cnKX"
      },
      "source": [
        "def graphIt(train, eval, name, enh, big=True):\n",
        "  import matplotlib.pyplot as plt\n",
        "  dir = \"/content/drive/My Drive/Colab Notebooks/datasets/SpamAssassin/train - test\"\n",
        "  fig, axes = plt.subplots(1, sharey=True, sharex=True, figsize=(12, 8))\n",
        "  axes.set_xlabel(\"Epoch\", fontsize=14)\n",
        "  axes.set_ylabel(\"Accuracy\", fontsize=14) \n",
        "\n",
        "  i = 0\n",
        "  sum = 0\n",
        "  sum_train = 0\n",
        "  for x in range (len(train)):\n",
        "    sum += eval[x]\n",
        "    sum_train += train[x]\n",
        "    i+=1\n",
        "\n",
        "  axes.plot(sum/(i), 'k--', linewidth=1.0, label=\"avg eval\")\n",
        "  axes.plot(sum_train/(i), 'k', linewidth=1.0, label=\"avg train\")\n",
        "\n",
        "  if(big==False):\n",
        "    plt.ylim([80, 100])\n",
        "  else:\n",
        "    plt.ylim([0, 100])\n",
        "  plt.xlim([0, 100])\n",
        "  plt.xticks(np.arange(0, 100, 5))\n",
        "  plt.legend()\n",
        "  plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcwHCFMbczJC"
      },
      "source": [
        "for item in types:\n",
        "  graphIt(accuracy_train[item], accuracy_eval[item], item, '-0.1', True)\n",
        "  graphIt(accuracy_train[item], accuracy_eval[item], item, '-0.1', False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
